<head>
  <title>Peeking Blackjack</title>
  <script src="plugins/main.js"></script>
  <script src="grader-auto.js"></script>
  <script src="grader-manual.js"></script>
</head>

<body onload="onLoad('blackjack', 'Andrew Han', 1)">

<div id="assignmentHeader"></div>

<p>
<img class="float-right" src="blackjack.jpg" style="width:260px;margin-left:10px"/>
</p>

<p>
The search algorithms explored in the previous assignment work great when you
know exactly the results of your actions.  Unfortunately, the real world is
not so predictable.  One of the key aspects of an effective AI is the ability
to reason in the face of uncertainty.  
</p>

<p>
Markov decision processes (MDPs) can be used to formalize uncertain situations.
In this homework, you will implement algorithms to find the optimal policy in these situations.
You will then formalize a modified version of Blackjack as an MDP, and apply your algorithm
to find the optimal policy.  
</p>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 1: Value Iteration</div>

<p>
In this problem, you will perform the value iteration updates manually on a
very basic game just to solidify your intuitions about solving MDPs.
The set of possible states in this game is {-2, -1, 0, 1, 2}.  You start at state 0, and if you reach either -2 or 2, the game ends.  
At each state, you can take one of two actions: {-1, +1}.
<p>
If you're in state $s$ and choose -1:
<ul> 
         <li> You have an 80% chance of reaching the state $s-1$.
         <li> You have a 20% chance of reaching the state $s+1$.
</ul>
If you're in state $s$ and choose +1:
<ul> 
         <li> You have a 30% chance of reaching the state $s+1$.
         <li> You have a 70% chance of reaching the state $s-1$.
</ul>

If your action results in transitioning to state -2,
then you receive a reward of 20.
If your action results in transitioning to state 2,
then your reward is 100.
Otherwise, your reward is -5.
Assume the discount factor $\gamma$ is 1.
</p>

<ol class="problem">

<li class="writeup" id="1a">
Give the value of $V_\text{opt}(s)$ for each state $s$ after 0, 1, and 2 iterations of value iteration. Iteration 0 just initializes all the values of $V$ to 0. Terminal states do not have any optimal polcies and take on a value of 0.
<div class="solution">
   <p>
      Iteration 0: <br>$V_\text{opt}(-2)$ = 0, <br>$V_\text{opt}(-1)$ = 0, <br>$V_\text{opt}(0)$ = 0, <br>$V_\text{opt}(1)$ = 0, <br>$V_\text{opt}(2)$ = 0<br>
      <br>
      Iteration 1: <br>$V_\text{opt}(-2)$ = 0,<br> $V_\text{opt}(-1) = \max\{0.8(20 + 0) + 0.2(-5 + 0), 0.3(-5 + 0) + 0.7(20 + 0) \} = 15$, <br> $V_\text{opt}(0) = \max\{0.8(-5 + 0) + 0.2(-5 + 0), 0.3(-5 + 0) + 0.7(-5 + 0) \} = -5$, <br> $V_\text{opt}(1) = \max\{0.8(-5 + 0) + 0.2(100 + 0), 0.3(100 + 0) + 0.7(-5 + 0) \} = 26.5$, <br>$V_\text{opt}(2)$ = 0<br>
      <br>
      Iteration 2: <br>$V_\text{opt}(-2)$ = 0,<br> $V_\text{opt}(-1) = \max\{0.8(20 + 0) + 0.2(-5 - 5), 0.3(-5 - 5) + 0.7(20 + 0) \} = 14$, <br> $V_\text{opt}(0) = \max\{0.8(-5 + 15) + 0.2(-5 + 26.5), 0.3(-5 + 26.5) + 0.7(-5 + 15) \} = 13.45$, <br> $V_\text{opt}(1) = \max\{0.8(-5 - 5) + 0.2(100 + 0), 0.3(100 + 0) + 0.7(-5 - 5) \} = 23$, <br>$V_\text{opt}(2)$ = 0<br>
   </p>
</div>

<li class="writeup" id="1b">
What is the resulting optimal policy $\pi_\text{opt}$ for all non-terminal states?
<div class="solution">
  $\pi_\text{opt}(-2) =$ no action (end state)<br>
  $\pi_\text{opt}(-1) = -1$<br>
  $\pi_\text{opt}(0) = +1$<br>
  $\pi_\text{opt}(1) = +1$<br>
  $\pi_\text{opt}(2) =$ no action (end state)
</div>

</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 2: Transforming MDPs</div>

<p>
Let's implement value iteration to compute the optimal policy on an arbitrary MDP.
Later, we'll create the specific MDP for Blackjack.
</p>

<ol class="problem">

<li class="code" id="2a">
If we add noise to the transitions of an MDP, does the optimal
value always get worse?
Specifically, consider an MDP with reward function
$\text{Reward}(s,a,s')$, states $\text{States}$, and transition function
$T(s,a,s')$.  Let's define a new MDP which is identical to the original, except
that on each action, with probability $\frac{1}{2}$, we randomly jump to one of the
states that we could have reached before with positive probability.
Formally, this modified transition function is:
$$T'(s,a,s')= \frac{1}{2} T(s,a,s') + \frac{1}{2} \cdot \frac{1}{|\{ s'' : T(s, a, s'') &gt; 0\}|}.$$
<p>
Let $V_1$ be the optimal value function for the
original MDP, and $V_2$ the optimal value function for the modified MDP.
Is it always the case that $V_1(s_\text{start})\geq
V_2(s_\text{start})$?  If so,
prove it in <code>blackjack.pdf</code> and put <code>return None</code> for each of the code blocks.
Otherwise, construct a counterexample by filling out <code>CounterexampleMDP</code>.
</p>

<div class="solution">
   <p>
   Counterexample: Define an MDP with three states:
   </p>
   <p>
   B &larr; A &rarr; C
   </p>

   <p>
   Let A be the start state and both B and C be terminal states.
   Assume there is only one action (say, 'travel') to perform from state A,
   which takes you to state B with reward 0 (probability 0.9)
   and state C with reward 10 (probability 0.1).
   Intuitively, if we add noise, it makes it more likely you'll end up in C and receive reward 10,
   so the optimal value goes up.
   This counterexample is coded up in <code>CounterexampleMDP</code>.
   </p>
</div>
</li>

<li class="writeup" id="2b">
Suppose we have an acyclic MDP.  We could run value iteration, which would require multiple iterations.
Briefly explain a more efficient algorithm that only requires one pass over all the $(s, a, s')$ triples.
<div class="solution">
  Since the MDP is acyclic, we can simply compute $V(s)$ by using the dynamic programming recurrence,
  which goes over each $(s, a, s')$ triple once.
  The reason that value iteration requires multiple passes is that we don't have an ordering over the states.
</div>
</li>

<li class="writeup" id="2c">
Suppose we have an MDP with states $\text{States}$ a discount factor $\gamma &lt; 1$,
but we have an MDP solver that only can solve MDPs with discount $1$.
How can leverage the MDP solver to solve the original MDP?
<p>
Let us define a new MDP with states $\text{States}' = \text{States} \cup \{ o \}$,
where $o$ is a new state.  Let's use the same actions ($\text{Actions}'(s) = \text{Actions}(s)$),
but we need to keep the discount $\gamma' = 1$.
Your job is to define new transition probabilities $T'(s, a, s')$ and rewards $\text{Reward}'(s, a, s')$
in terms of the old MDP
such that the optimal values $V_\text{opt}(s)$ for all $s \in \text{States}$
are the equal under the original MDP and the new MDP.
<div class="solution">
  The idea is to interpret the discount $\gamma$ as the probability of not transitioning into $o$. Let $o$ be a terminal state.  This admits two solutions:
  <ol>
  <li> (correct)
    <ul> 
      <li> $T'(s, a, s') \doteq \gamma T(s, a, s')$ for $s' \in \text{States}$ </li>
      <li> $T'(s, a, o) \doteq 1 - \gamma$ </li>
      <li> $\text{Reward}'(s, a, s') \doteq \text{Reward}(s, a, s')$ for all $s' \in \text{States}$ </li>
      <li> $\text{Reward}'(s, a, o) \doteq \sum_{s' \in \text{States}} T(s, a, s') \text{Reward}(s, a, s')$ </li>
      </ul>
  </li>
  
  <li> (correct)
    <ul> 
      <li> $T'(s, a, s') \doteq \gamma T(s, a, s')$ for $s' \in \text{States}$ </li>
      <li> $T'(s, a, o) \doteq 1 - \gamma$ </li>
      <li> $\text{Reward}'(s, a, s') \doteq \frac{1}{\gamma} \text{Reward}(s, a, s')$ for all $s' \in \text{States}$ </li>
      <li> $\text{Reward}'(s, a, o) \doteq 0$ </li>
      </ul>
  </li>  

  <li> (incorrect)
    <ul> 
      <li> $T'(s, a, s') \doteq \gamma T(s, a, s')$ for $s' \in \text{States}$ </li>
      <li> $T'(s, a, o) \doteq 1 - \gamma$ </li>
      <li> $\text{Reward}'(s, a, s') \doteq \text{Reward}(s, a, s')$ for all $s' \in \text{States}$ </li>
      <li> $\text{Reward}'(s, a, o) \doteq 0$ </li>
      </ul>
  </li>

</ol>
  <p>
  Recall the recurrence for the new optimal value:
  $$V_\text{opt}'(s) = \max_{a \in \text{Actions}(s)} \sum_{s' \in \text{States}'} T'(s, a, s') [\text{Reward'}(s, a, s') + V_\text{opt}'(s')].$$
  $$ = \max_{a \in \text{Actions}(s) \setminus o} \sum_{s' \in \text{States}'} T'(s, a, s') [\text{Reward'}(s, a, s') + V_\text{opt}'(s')]$$
   $$+ T'(s, a, o) [\text{Reward'}(s, a, o) + V_\text{opt}'(o)] $$
  Plugging in the definitions of the new transitions and rewards, we get that, for each solution:
  <ol>
    <li>
    $$V_\text{opt}'(s) = \max_{a \in \text{Actions}(s)} \sum_{s' \in \text{States}} \gamma T(s, a, s') [\text{Reward}(s, a, s') + V_\text{opt}'(s')] + \\ (1 - \gamma) \sum_{s' \in \text{States}} T(s, a, s') \text{Reward}(s, a, s'),$$
    $$ = \max_{a \in \text{Actions}(s)} \sum_{s' \in \text{States}} T(s, a, s') [\text{Reward}(s, a, s') + \gamma V_\text{opt}'(s')].$$
    </li>

      <li>
    $$V_\text{opt}'(s) = \max_{a \in \text{Actions}(s)} \sum_{s' \in \text{States}} \gamma T(s, a, s') [\frac{1}{\gamma} \text{Reward}(s, a, s') + V_\text{opt}'(s')] + \\ (1 - \gamma) *0 $$
    $$ = \max_{a \in \text{Actions}(s)} \sum_{s' \in \text{States}} T(s, a, s') [\text{Reward}(s, a, s') + \gamma  V_\text{opt}'(s')] $$

    </li>        
   <li>
    $$V_\text{opt}'(s) = \max_{a \in \text{Actions}(s)} \sum_{s' \in \text{States}} \gamma T(s, a, s') [\text{Reward}(s, a, s') + V_\text{opt}'(s')] + \\ (1 - \gamma) * 0$$
    $$ =  \max_{a \in \text{Actions}(s)} \sum_{s' \in \text{States}} T(s, a, s') [\gamma \text{Reward}(s, a, s') + \gamma V_\text{opt}'(s')]$$    
    </li>

 </ol>
  In all cases except for the third, we have recovered the original recurrence:
      $$V_\text{opt}'(s) = \max_{a \in \text{Actions}(s)} \sum_{s' \in \text{States}} T(s, a, s') [\text{Reward}(s, a, s') + \gamma V_\text{opt}'(s')].$$
    Therefore, the new MDP and the old MDP have the same optimal values.
</div>
</li>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 3: Peeking Blackjack</div>

<p>
Now that we have written general-purpose MDP algorithms, let's use them
to play (a modified version of) Blackjack.
For this problem, you will be creating an MDP to describe a modified version
of Blackjack.
</p>

<p>
For our version of Blackjack, the deck can contain an
arbitrary collection of cards with different values, each with a given
multiplicity.  For example, a standard deck would have card values $[1, 2, \ldots, 
13]$ and multiplicity 4.  You could also have a deck with card values
$[1,5,20]$.
The deck is shuffled (each permutation of the cards is equally likely).
</p>

<p>
The game occurs in a sequence of rounds.
Each round, the player either
(i) takes the next card from the top of the deck (costing nothing),
(ii) peeks at the top card
(costing <code>peekCost</code>, in which case the next round, that card will be drawn),
or (iii) quits the game.
(Note: it is not possible to peek twice in a row; if the player peeks twice in a row, then
<code>succAndProbReward()</code> should return <code>[]</code>.)
</p>

<p>
The game continues until one of the following conditions becomes true:
<ul>
   <li>The player quits, in which case her reward is the sum of the cards in her hand.
   <li>The player takes a card, and this leaves her with a sum that is strictly greater than the
threshold, in which case her reward is 0.
   <li>The deck runs out of cards, in which case it is as if she quits, and she
gets a reward which is the sum of the cards in her hand.
</ul>
</p>

<p>
In this problem, your state $s$ will be represented as a triple:
<blockquote>
  <code>(totalCardValueInHand, nextCardIndexIfPeeked, deckCardCounts)</code>
</blockquote>
As an example, assume the deck has card values $[1, 2, 3]$ with multiplicity 1,
and the threshold is 4.
Initially, the player has no cards, so her total is 0;
this corresponds to state <code>(0, None, (1, 1, 1))</code>.
At this point, she can take, peek, or quit.
<ul>
  <li>If she takes, the three possible 
successor states (each has $1/3$ probability) are
<blockquote>
  <code>(1, None, (0, 1, 1))</code><br>
  <code>(2, None, (1, 0, 1))</code><br>
  <code>(3, None, (1, 1, 0))</code><br>
</blockquote>
She will receive reward 0 for reaching any of these states.
<li>
If she instead peeks, the three possible successor states are
<blockquote>
  <code>(0, 0, (1, 1, 1))</code><br>
  <code>(0, 1, (1, 1, 1))</code><br>
  <code>(0, 2, (1, 1, 1))</code><br>
</blockquote>
She will receive reward <code>-peekCost</code> to reach these states.
From <code>(0, 0, (1, 1, 1))</code>, taking yields <code>(1, None, (0, 1, 1))</code> deterministically.
<li>
If she quits, then the resulting state will be <code>(0, None, None)</code>
(note setting the deck to <code>None</code> signifies the end of the game).
</ul>
</p>
<p>
As another example, let's say her current state is <code>(3, None, (1, 1, 0))</code>.
<ul>
  <li>If she quits, the successor state will be <code>(3, None, None)</code>.
  <li>If she takes, the successor states are <code>(3 + 1, None, (0, 1, 0))</code> or <code>(3 + 2, None, None)</code>.
Note that in the second successor state, the deck is set to <code>None</code> to signify the game ended with a bust.
You should also set the deck to <code>None</code> if the deck runs out of cards.  
</ul>
</p>

<ol class="problem">

<li class="code" id="3a">
Implement the game of Blackjack as an MDP by filling out the
<code>succAndProbReward()</code> function of class <code>BlackjackMDP</code>.
</li>

<li class="code" id="3b">
Let's say you're running a casino, and you're trying to design a deck to make
people peek a lot.  Assuming a fixed threshold of 20, and a peek cost of 1,
design a deck where for at least 10% of states, the optimal policy is to peek.
Fill out the function <code>peekingMDP()</code> to return an instance of
<code>BlackjackMDP</code> where the optimal action is to peek in at least
10% of states.
</li>

</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 4: Learning to Play Blackjack</div>

<p>
So far, we've seen how MDP algorithms can take an MDP which describes the full
dynamics of the game and return an optimal policy.  But suppose you go into
a casino, and no one tells you the rewards or the transitions.
We will see how reinforcement learning can allow you to play the game
and learn the rules at the same time!
</p>

<ol class="problem">

<li class="code" id="4a">
You will first implement a generic Q-learning algorithm <code>QLearningAlgorithm</code>,
which is an instance of an <code>RLAlgorithm</code>.  As discussed in class,
reinforcement learning algorithms are capable of executing a policy while
simultaneously improving their policy.  Look in <code>simulate()</code>, in
<code>util.py</code> to see how the <code>RLAlgorithm</code> will be used.  In
short, your <code>QLearningAlgorithm</code> will be run in a simulation of the MDP, and will
alternately be asked for an action to perform in a given state (<code>QLearningAlgorithm.getAction</code>), and then be
informed of the result of that action (<code>QLearningAlgorithm.incorporateFeedback</code>),
so that it may learn better actions to perform in the future.
</p>

<p>
We are using Q-learning with function approximation,
which means $\hat Q_\text{opt}(s, a) = \mathbb w \cdot \phi(s, a)$,
where in code, $\mathbb w$ is <code>self.weights</code>, $\phi$ is the <code>featureExtractor</code> function,
and $\hat Q_\text{opt}$ is <code>self.getQ</code>.
<p>
We have implemented <code>QLearningAlgorithm.getAction</code> as a simple $\epsilon$-greedy policy.
Your job is to implement <code>QLearningAlgorithm.incorporateFeedback()</code>,
which should take an $(s, a, r, s')$ tuple and update <code>self.weights</code>
according to the standard Q-learning update.
</p>
</li>

<li class="writeup" id="4b">
Call <code>simulate</code> using your algorithm and the
<code>identityFeatureExtractor()</code> on the MDP <code>smallMDP</code>, with
30000 trials. Compare the policy learned in this case to the policy
learned by value iteration.  Don't forget to set the explorationProb of your
Q-learning algorithm to 0 after learning the policy.  How do the two policies
compare (i.e., for how many states do they produce a different action)?  Now
run <code>simulate()</code> on <code>largeMDP</code>.  How does the policy
learned in this case compare to the policy learned by value iteration?  What
went wrong?

<div class="solution">
   <p>
   Differences in actions will vary depending on implementation.  For smallMDP, the difference should be less than 8%.  For largeMDP, the difference should be around 30%.
   </p>
   <p>
   Q-learning does better on smallMDP because the state space is relatively small.  This allows the Q-learning algorithm to better learn the Q values for each (state, action) pair.
   </p>
   <p>
   Q-learning does worse on largeMDP because the state space is much larger.  This means that the Q-learning algorithm is not able to learn accurate values for each (state, action) pair. This problem is compounded by the fact that our identityFeatureExtractor is unable to describe the value at unseen states. 
   </p>
   
   <p>
      Common mistake: Many people only commented on the feature extractor and did not mention the large state space created by largeMDP.
   </p>
</div>

</li>

<li class="code" id="4c">
To address the problems explored in the previous exercise, we incorporate
domain knowledge to improve generalization.  This way, the algorithm can use
what it learned about some states to improve its prediction performance on
other states. Implement <code>blackjackFeatureExtractor</code> as described in the code comments.
Using this feature extractor, you should be able to get pretty close to the
optimum on the <code>largeMDP</code>.
</li>

<li class="writeup" id="4d">
Now let's explore the way in which value iteration responds to a change in the
rules of the MDP.  Run value iteration on <code>originalMDP</code> to compute an
optimal policy.  Then apply your policy to <code>newThresholdMDP</code> by
calling <code>simulate</code> with <code>FixedRLAlgorithm</code>, instantiated
using your computed policy.  What reward do you get? What happens if you run Q
learning on <code>newThresholdMDP</code> instead? Explain.

<div class="solution">

  You get relatively low rewards for <code>FixedRLAlgorithm</code> because you are passing
  in the policy learned for originalMDP.  Because <code>FixedRLAlgorithm</code> doesn't
  adapt, the actions taken are not optimal actions for <code>newThresholdMDP</code>.

   <p>
   Q-learning has higher rewards because it is able to adapt to <code>newThresholdMDP</code>.
   </p>

   <p>
      Common mistakes:
      <ul>
         <li>Forgot to mention that Q-learning can adapt.
         <li>Got incorrect average rewards.
         <li>Reported rewards did not make sense.  (Some people reported that the reward for Q-learning was always 12 (or they didn't specify that it could be a different value from 12).  This is incorrect, as the rewards returned by <code>simulate(newThresholdMdp, QLearningAlgorithm, ...)</code> are often less than 12.)
      </ul>
   </p>  
 
</div>

</li>

</ol>

</body>
