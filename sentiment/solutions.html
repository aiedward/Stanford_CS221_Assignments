<head>
  <title>Sentiment Analysis</title>
  <script src="plugins/main.js"></script>
  <script src="grader-auto.js"></script>
  <script src="grader-manual.js"></script>

<link rel="stylesheet" type="text/css" href="plugins/main.css" />
</head>

<body onload="onLoad('sentiment', 'Isaac Caswell', 1)">
<div id="assignmentHeader"></div>

<!------------------------------------------------------------>

Advice for this homework:
<ol class="problem">
  <li> Words are simply strings separated by whitespace.  Don't normalize the capitalization of words (treat <i>great</i> and <i>Great</i> as different words).
  <li> You might find some useful functions in <code>util.py</code>.  Have a look around in there before you start coding.
</ol>

<div class="problemTitle">Problem 1: Warmup</div>

<p>
Here are two reviews of "Frozen," courtesy of <a href="https://www.rottentomatoes.com">Rotten Tomatoes</a> (no spoilers!):
</p>

<p>
<img src="posreview.jpg" />
<img src="negreview.jpg" />
</p>


<p>
Rotten Tomatoes has classified these reviews as "positive" and "negative," respectively, as indicated by the in-tact tomato
on the left and the splattered tomato on the right.  In this assignment, you will create a simple text classification system
that can perform this task automatically.
</p>

<ol class="problem">
	We'll warm up with the following set of four mini-reviews, each labeled positive (+1) or negative (-1):
	<ol>
	<li>(+1) pretty good</li>
	<li>(-1) bad plot</li>
	<li>(-1) not good</li>
	<li>(+1) pretty scenery</li>
	</ol>
  Each review $x$ is mapped onto a feature vector $\phi(x)$,
  which maps each word to the number of occurrences of that word in the review.
  For example, the first review maps to the (sparse) feature vector $\phi(x) = \{\text{pretty}:1, \text{good}:1\}$.
  Recall the definition of the hinge loss:
  $$\text{Loss}_{\text{hinge}}(x, y, \mathbf{w}) = \max \{0, 1 - \mathbf{w} \cdot \phi(x) y\},$$
	where $y$ is the correct label.

  <li class="writeup" id="1a"> Suppose we run stochastic gradient descent, updating the weights according to
  $$\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla_\mathbf{w} \text{Loss}_{\text{hinge}}(x, y, \mathbf{w}),$$
  once for each of the four examples in order.
	After the classifier is trained on the given four data points, what are the weights of the six words ('pretty', 'good', 'bad',
	'plot', 'not', 'scenery') that appear in the above reviews?  Use $\eta = 1$ as the step size and initialize $\mathbf{w} = [0, ..., 0]$.
  Assume that $\nabla_\mathbf{w} \text{Loss}_{\text{hinge}}(x, y, \mathbf{w}) = 0$ when the margin is exactly 1.
	</li>
	<div class="solution">
		<ol type="1">
		<li> (score=0, loss=1) {pretty: 1, good: 1}
		<li> (score=0, loss=1) {pretty: 1, good: 1, bad: -1, plot: -1}
    <li> (score=-1, loss=2) {pretty: 1, good: 0, bad: -1, plot: -1, not: -1}
    <li> (score=1, loss=0) {pretty: 1, good: 0, bad: -1, plot: -1, not: -1, scenery: 0}
		</ol>
		(Only this last line needs to be included in the given answer) <br>
    NOTE: If they ended up with the following solution, they may have copied off last year's solution, which uses 'not bad' instead of 'not good':
    <br> (score=1, loss=0) {pretty: 1, good: 1, bad: 0, plot: -1, not: 1, scenery: 0}
	</div>

  <li class="writeup" id="1b">
  Create a small labeled dataset of four mini-reviews using the words 'not', 'good', and 'bad',
  where the labels make intuitive sense.
  Each review should contain one or two words, and no repeated words.
	Prove that no linear classifier using word features can get zero error on your dataset. Remember that this is a question about classifiers, not optimization algorithms: your proof should be true for any linear classifier, regardless of how the weights are learnt. <br>
   After providing such a dataset, propose a single additional feature that we could augment the feature vector with that would fix this problem.
  (Hint: think about the linear effect that each feature has on the classification score.)
  </li>
  <div class="solution">
  <ol type="1">
    <li> (-1) 'bad'
    <li> (+1) 'good'
    <li> (+1) 'not bad'
    <li> (-1) 'not good' </li>

  Proof: to classify 'bad' and 'good' correctly, they must have negative and positive weights, respectively.  But then if 'not' has
  a positive weight then 'not good' is classified incorrectly, and if 'not' has a negative weight then 'not bad' is classified
  incorrectly.

  A single redeeming feature would be (for example) an indicator feature for the phrase 'not good'.  Then the following
  weight vector classifies all the examples correctly:
  $$\mathbf{w} = \{\text{'bad'}:-1, \text{ 'good'}:1, \text{ 'not'}:2, \text{ 'not good'}:-4\}$$
  </ol>
  </div>
</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 2: Predicting Movie Ratings</div>
<ol class="problem">

  <p>
    Suppose that we are now interested in predicting a numeric rating for each movie review.
    We will use a non-linear predictor that takes a movie review $x$ and returns $\sigma(\mathbf w \cdot \phi(x))$,
    where $\sigma(z) = (1 + e^{-z})^{-1}$ is the logistic function that squashes a real number to the range $[0, 1]$.
    Suppose that we wish to use the squared loss.
  </p>

  <li class="writeup" id="2a">
    Write out the expression for $\text{Loss}(x, y, \mathbf w)$.
  </li>
  <div class="solution">
    $$\text{Loss}(x, y, \mathbf w) = (\sigma(\mathbf w \cdot \phi(x)) - y)^2.$$
  </div>

  <li class="writeup" id="2b">
    Compute the gradient of the loss. <br>
    Hint: you can write the answer in terms of the predicted value $p = \sigma(\mathbf w \cdot \phi(x))$.
  </li>
  <div class="solution">
    $$\nabla \text{Loss}(x, y, \mathbf w) = 2 (p - y) p (1 - p) \phi(x),$$
    where $p = \sigma(\mathbf w \cdot \phi(x))$ is the predicted value.
    Importantly, $p \in [0, 1]$.
  </div>

  <li class="writeup" id="2c">
    Assuming $y = 0$, what is the smallest magnitude that the gradient can take?
    That is, find a way to set $\mathbf w$ to make $\|\nabla \text{Loss}(x, y, \mathbf w)\|$ as small as possible.
    You are allowed to let the magnitude of $\mathbf w$ go to infinity.
    <i>Hint: try to understand intuitively what is going on and the contribution of each part of the expression.
      If you find doing too much algebra, you're probably doing something suboptimal.</i>
    <p>Motivation: the reason that we're interested in the magnitude of the gradients is because it governs
    how far gradient descent will step.  For example, if the gradient is close to zero when $\mathbf w$
    is very far from the origin, then it could take a long time for gradient descent to reach the optimum (if at all);
    this is known as the vanishing gradient problem in training neural networks.</p>
  </li>
  <div class="solution">
    To make the gradient small, we make $p$ tend to $1$ or $0$,
    by taking $\mathbf w$ to be a ever larger positive ($\infty$) or negative ($-\infty$) multiple of $\phi(x)$ respectively.
    So the minimum magnitude of the gradient is $0$.
  </div>

  <li class="writeup" id="2d">
    Assuming $y = 0$, what is the largest magnitude that the gradient can take? Leave your answer in terms of $\|\phi(x)\|$.
  </li>
  <div class="solution">
    To make the gradient large, we want to maximize $p^2 (1-p)$, which
    by the foundations homework coin-flip-calculation is obtained by setting $p = \frac{2}{3}$, which yields $\frac{4}{27}$.
    Therefore, the maximum magnitude is $2 (\frac{4}{27}) \|\phi(x)\| = \frac{8}{27} \|\phi(x)\|$.
    <p>
    This exercise hints at the vanishing gradient problem in training neural networks,
    where the gradient is (close to) zero even when the weights $\mathbf w$ are
    far away from the global optimum ($p$ close to $0$ or $1$).
    However, the gradient tends to be large when $p$ is closer to the center.
    The exact value is due to the fact that $y = 0$.
  </div>

  <li class="writeup" id="2e">
    The problem with the loss function we have defined so far is that is it is non-convex,
    which means that gradient descent is not guaranteed to find the global minimum,
    and in general these types of problems can be difficult to solve.
    So let us try to reformulate the problem as plain old linear regression.
    Suppose you have a dataset $\mathbf D$ consisting of $(x,y)$ pairs, and that there exists a weight vector $\mathbf w$ that yields zero loss on this dataset.
    Show that there is an easy transformation
    to a modified dataset $\mathbf D'$ of $(x,y')$ pairs such that performing least squares regression (using a linear predictor and the squared loss) on $\mathbf D'$ converges to a vector $\mathbf w^*$ that yields zero loss on $\mathbf D'$.
    <!-- that yields the same loss as $\mathbf w$ does on $\mathbf D$. -->
    Concretely, write an expression for $y'$ in terms of $y$ and justify this choice. This expression should not be a function of $\mathbf w$. <br>
    <i> &#9758; For this part of the problem, assume that $y$ is a real valued variable in the range (0, 1).</i>
  </li>
  <div class="solution">
    Transform each training point to $(x,y)$ to $(x,y')$, where
    $$y' = \sigma^{-1}(y) = \log\left(\frac{y}{1-y}\right)$$
    inverts the transformation $\sigma$ (which is monotonic).
    Now suppose $\mathbf w$ obtains zero loss on $(x,y')$,
    which means that $(\mathbf w \cdot \phi(x) - y')^2 = 0$,
    or equivalently, $\mathbf w \cdot \phi(x) = y' = \sigma^{-1}(y)$.
    Applying $\sigma$ to both sides yields
    $\sigma(\mathbf w \cdot \phi(x)) = y$,
    which is saying that $\mathbf w$ also obtains zero loss on the original example $(x,y)$.
  </div>

</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 3: Sentiment Classification</div>

<p>
<img src="sentiment.jpg" />
</p>

In this problem, we will build a binary linear classifier that reads movie
reviews and guesses whether they are "positive" or "negative."

<ol class="problem">

	<li class="code" id="3a"> Implement the function <code>extractWordFeatures</code>, which takes a review (string) as input and returns a feature vector
	$\phi(x)$ (you should represent the vector $\phi(x)$ as a <code>dict</code> in Python).
	</li>

	<li class="code" id="3b"> Implement the function <code>learnPredictor</code> using stochastic gradient descent, minimizing
    the hinge loss.
  Print the training error and test error after each iteration through the data,
  so it's easy to see if your code is working.
  You must get less than 4% error rate on the training set and less than 30%
  error rate on the dev set to get full credit.
  </li>

	<li class="code" id="3c"> Create an artificial dataset for your <code>learnPredictor</code> function by
    writing the <code>generateExample</code> function (nested in the <code>generateDataset</code> function).
    Use this to double check that your <code>learnPredictor</code> works!
  </li>

	<li class="writeup" id="3d">
  When you run the grader.py on test case <code>3b-2</code>, it should output a <code>weights</code>
  file and a <code>error-analysis</code> file.
  Look through 10 example incorrect predictions and for each one,
  give a one-sentence explanation of why the classification was incorrect.
  What information would the classifier need to get these correct?
  In some sense, there's not one correct answer, so don't overthink this problem;
  the main point is to get you to get intuition about the problem.
  </li>
  <div class="solution">
    Here's an example of a positive review that was misclassified as negative:
    <p><i>wickedly funny , visually engrossing , never boring , this movie challenges us to think about the ways we consume pop culture .</i></p>
    <p>After looking at the weights in the error-analysis, we find that positive words such as "engrossing" and "funny" are outweighed by negative words such as "never" and "boring" although "never boring" together represent a positive sentiment. Additional information such as which words appear together would help the classifier assign a positive weight to "never boring" and handle negations somewhat more robustly. The idea here is similar to <code>extractCharacterFeatures</code> but instead of extracting n-grams of characters, we would add features that are n-grams of words. For example, counts of bigrams (2 consecutive words) could be added to the feature set.</p>
  </div>

  <li class="code" id="3e">
  Now we will try a crazier feature extractor.  Some languages are written without spaces between words.
  But is this step really necessary, or can we just naively consider strings of characters that stretch across words?
  Implement the function <code>extractCharacterFeatures</code>
  (by filling in the <code>extract</code> function), which maps each string of $n$ characters
  to the number of times it occurs, ignoring whitespace (spaces and tabs).

	<li class="writeup" id="3f"> Run your linear predictor with feature extractor <code>extractCharacterFeatures</code>.  Experiment
    with different values of $n$ to see which one produces the smallest test error.  You should observe that this error is
    nearly as small as that produced by
    word features.  How do you explain this?
    <p>Construct a review (one sentence max) in which character $n$-grams
    probably outperform word features, and briefly explain why this is so.</p>
    <div class="solution">
      One possible answer: 4 characters is exactly enough to catch a "not."  The 4-gram "notg" is probably "not good," which should have a negative weight,
      while the 4-gram "notb" is probably "not bad" and should have a positive weight.  Word features would miss this distinction.
    </div>
  </li>

</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 4: K-means clustering</div>

Suppose we have a feature extractor $\phi$ that produces 2-dimensional feature
vectors, and a toy dataset $\mathcal D_\text{train} = \{x_1, x_2, x_3, x_4\}$ with
<ol type="1">
<li> $\phi(x_1) = [0, 0]$
<li> $\phi(x_2) = [0, 1]$
<li> $\phi(x_3) = [2, 0]$
<li> $\phi(x_4) = [2, 2]$
</ol>

<ol class="problem">
  <li class="writeup" id="4a">
  Run 2-means on this dataset.  Please show your work. What are the final cluster assignments $z$ and cluster centers $\mu$?
  Run this algorithm twice, with initial centers:
  <ol type="1">
  <li> $\mu_1 = [-1, 0]$ and $\mu_2 = [3, 2]$
  <li> $\mu_1 = [1, -1]$ and $\mu_2 = [0, 2]$
  </ol>
  <div class="solution">
  <ol type="1">
  <li> Initial clusterings: $\mu_1$ has $\{x_1, x_2\}$ and $\mu_2$ has $\{x_3, x_4\}$.
       Then we set $\mu_1 = [0, \frac{1}{2}]$, and $\mu_2 = [2, 1]$.
       This is stable.
  <li> Initial clusterings: $\mu_1$ has $\{x_1, x_3\}$ and $\mu_2$ has $\{x_2, x_4\}$.
        Then we set $\mu_1 = [1, 0]$ and $\mu_2 = [1, \frac{3}{2}]$.
        This is stable.
<hr>
  <br> <b>LAST YEAR'S SOLUTIONS (WRONG):</b>
    <li> Initial clusterings: $\mu_1$ has $\{x_1, x_3\}$ and $\mu_2$ has $\{x_2, x_4\}$.
       Then we set $\mu_1 = [\frac{1}{2}, 0]$, and $\mu_2 = [1, \frac{3}{2}]$.
       This is stable.
  <li> Initial clusterings: $\mu_1$ has $\{x_1, x_2\}$ and $\mu_2$ has $\{x_3, x_4\}$.
        Then we set $\mu_1 = [\frac{3}{2}, \frac{1}{2}]$ and $\mu_2 = [0, 1]$.
        This is stable.
  </ol>
  </div>

  <li class="code" id="4b">
  Implement the <code>kmeans</code> function.  You should initialize your $k$
  cluster centers to random elements of <code>examples</code>.

  After a few iterations of k-means, your centers will be very dense vectors.
  In order for your code to run efficiently and to obtain full credit, you will
  need to precompute certain quantities. As a reference, our code runs in under
  a second on Myth, on all test cases. You might find <code>generateClusteringExamples</code>
  in util.py useful for testing your code.

  </li>

  <li class="writeup" id="4c">
  Sometimes, we have prior knowledge about which points should belong in the same cluster.
  Suppose we are given a set $S$ of example pairs
  $(i,j)$ which must be assigned to the same cluster.
  For example, suppose we have 5 examples; then $S = \{ (1, 2), (1, 4), (3, 5) \}$ says that examples 1, 2, 4 must be in the same cluster
  and that examples 3 and 5 must be in the same cluster.
  Provide the modified k-means algorithm that performs alternating minimization
  on the reconstruction loss. <br>
  <div class="solution">
    Recall that the reconstruction loss is given by:
    $$\sum \limits_{i=1}^n \| \mu_{z_i} - \phi(x_i) \|^2$$
    <p>
    When $z$ is fixed and we are optimizing $\mu$, we get that each $\mu$ should be assigned to the average of the
    points currently assigned to it (this is no different from the usual k-means update step).  Proof: the derivative
    of the reconstruction loss with respect to any one centroid $\mu_j$ is
    $$2 \sum \limits_{x \text{ is in } \mu_j \text{'s cluster}} (\mu_j - \phi(x))$$
    To minimize, we set this equal to zero, and we get $$\mu_j = \frac{\sum \limits_{x \text{ is in } \mu_j \text{'s cluster}} \phi(x)}{|\{x \, | \, x \text{ is in } \mu_j \text{'s cluster}\}|}$$
    </p>
    <p>
    When $\mu$ is fixed and we are optimizing over $z$, we need to do something new.  There are two steps to this process.
    First, note that if $(i,j) \in S$ and $(j,k) \in S$, then $i$ and $k$ must also be in the same cluster (by transitivity).
    Therefore, $S$ really partitions the data points into disjoint groups.
    Let $g_i$ denote the group that $i$ is assigned to; this can be computed using the Union-Find algorithm.
    </p>
    <p>
    Now, the cluster assignments for each group must be done jointly.
    Specifically, we have
    $$z_i = \arg \min \limits_{z \in \{1, \dots, k\}} \sum \limits_{j : g_i = g_j} \|\phi(x_j) - \mu_z\|^2$$
    </p>
  </div>
</ol>
